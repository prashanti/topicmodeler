{\rtf1\ansi\ansicpg1252\cocoartf949\cocoasubrtf540
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww11400\viewh15200\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\b\fs32 \cf0 LIST OF OUTPUT FILES FOR TEXT VISUALIZATION TOOLBOX (TVT)
\fs24 \
\
NOTE TO USER\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\ql\qnatural\pardirnatural

\b0 \cf0 The following paragraphs describe the files output by the TVT (Text Visualization Toolbox) matlab toolbox.  The files that may be read to view outputs and understand each step of the analysis are the .tab, .txt, and . html files.  The .mat files are matlab files that are generally useful for performing the analyses (i.e., they contain inputs and outputs for the various matlab scripts), and are not intended to be viewed directly; descriptions of these files are included to aid in troubleshooting.
\b \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural
\cf0 \
pedigree.html files\

\b0 Many files output by tvt have a corresponding pedigree file.  These pedigree files are automatically generated whenever the tvtExecute.m script is run, and they serve as documentation of which files and parameters were used to create the corresponding output.  A pedigree file with the current date and time is created for each potential output, even if not all outputs are actually requested.\
\

\b 00errors_log.tab
\b0 \
This tabbed text file is created by several of the tvt scripts (makedictfromtab.m, makeTMT.m, reduceTMT.m, and trainSOM.m) , and all of the scripts report identified errors to this file (additional error reports are appended as more scripts are run).  Errors that can be reported include: data fields missing from all documents in an input matrix; data fields missing from individual documents; metafield tags found within the text field when the corresponding metafield contains no data.  A report of the quantization error from the sompak training phase is also appended to this file.\
\

\b 01dictionary+....txt\

\b0 The dictionary is a tab-separated file created by makedictfromtab.m, which provides the code to convert words (tokens) into numbers:\
- The first row contains the date and time the dictionary was created\
- Each subsequent row corresponds to one word and contains 3 columns:\
	-1st column - word/token\
	-2nd column - unique numerical identifier for the word/token\
	-3rd column - number of times the word appears in the corpus\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\ql\qnatural\pardirnatural

\b \cf0 newspapers20081121to20090131_0001-0499.mat\
newspapers20081121to20090131_0500-0999.mat
\b0 \
These are the matlab files that contain data structures that correspond to the tabbed text input files.  Each of these files contains a series of numbers that represents the sequence of tokens (words) for each document in the input file of the same name.  The correspondence between tokens and numbers is provided by the 01dictionary+....txt file.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural
\cf0 \
NOTE:  The creation of the dictionary and the conversion of the tab-separated input files into the .mat files above uses several parameters that are specified in tvtParameters.m.  More details can be discovered by typing 'makedictfromtab help' at the matlab prompt.\
\
\

\b 02TMT+....mat
\b0 \
This matlab file contains data structures in the TMT (Topic Modeling Toolbox) format that represent the whole corpus of documents.  This file is created by makeTMT.m, which also removes duplicate documents.  This file is in the format necessary for the Latent Dirichlet Allocation (LDA) and Latent \
Dirichlet Allocation with Collocations (LDACOL) algorithms.  \
\

\b 03redTMT+....mat
\b0 \
This matlab file contains the data structure generated by reduceTMT.m, and is in the format necessary for the Latent Dirichlet Allocation (LDA) and and Latent Dirichlet Allocation with Collocations (LDACOL) algorithms.  This file is a simplified version of the 02TMT+....mat file in which several types of tokens or documents have been removed to reduce the computational intensity of the LDA algorithm and improve interprability of the LDA outputs:\
	1) It removes a specified list of stopwords (in the example case, the included ENstoplist.txt file was used).  The .txt file to be used as a stoplist is specified in the tvtParameters.m file.\
	2) It removes words (tokens) whose global frequency in the corpus is below a specified threshold (in the example case, a threshold value of 10 was used). The global frequency threshold is specified in the tvtParameters.m file.\
	3) If a classification algorithm was used to select documents of interest for a particular analysis, those documents in classes deemed to be of no interest will be excluded from this file.  The tab-separated text file that reports the classification used for document exclusion is specified in the tvtParameters.m file.\
More details about the parameters can be discovered by typing 'reduceTMT help' at the matlab prompt.\
\

\b 04ldacol+...+nIter=....mat\

\b0 This matlab file is produced by ldacol,m and contains the data structures that includes all inputs to and outputs from the TMT script GibbsSamplerLDACOL.  (The inputs to this script are actually the outputs of 03redTMT+....m). The outputs contained with this matlab file include the following variables: \
	1) the matrix ''dataMatrix'' with the topic weights for each document (one document per row, one topic per column), \
	2) the array ''nameDocs'' with documents names in the form xxxx(99) where ''xxxx'' represents the name of the .tab (raw) file containing the document and ''99'' represents the order of the document within that file,\
	3) the array ''nameCols'' with topic descriptions (i.e., the top words in each topic),\
	4) the array ''metaDocs'' with the documents metadata (one document per row, one metadata field per column).\
Generally there are several different versions of this file, which correspond to outputs from different numbers of iterations of the GibbsSamplerLDACOL algorithm.  While the file with the largest number of iterations should be the most accurate, outputs based on different numbers of iterations can be used to  understand how well the algorithm is converging.\
The number of topics to be discovered by the GibbsSamplerLDACOL algorithm and the number of iterations of the algorithm are specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'ldacol help' at the matlab prompt.\
\

\b 04ldacol+...+nIter=...+topics.txt
\b0 \
This tab-separated file is produced by ldacol.m and includes information about the words and/or collocations associated with each of the extracted latent topics.  The number of topics extracted and the number of iterations are among the input parameters to the LDACOL algorithm.  Topic numbers assigned to the latent topics are displayed in the first column followed by the words/collocations most frequently associated with that topic.  The second column contains values that represent the overall weight of the topic within the corpus and the weight of each of the words/collocations within the associated topic.  Generally there are several different versions of this file, which correspond to outputs from different numbers of iterations of the GibbsSamplerLDACOL algorithm.  While the file with the largest number of iterations should be the most accurate, outputs based on different numbers of iterations can be used to  understand how well the algorithm is converging.\
\

\b 05createSOM+....mat\

\b0 This matlab file is produced by createSOM.m and is a simplified version of the 04ldacol+...+nIter=....mat file described above.  For this example, no documents or topics have been excluded, but in the case that exclusions are desired, tab-separated files identifying the documents or topics to be excluded are specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'createSOM help' at the matlab prompt.\
\

\b 05createSOM+...+train1/2/3.txt
\b0 \
These tab-separate text files contain versions of the matrix 'dataMatrix' in a format readble by SOM_PAK:\
	1) The first row contains the number of columns (i.e., number of topics)\
	2) Each subsequent row corresponds to one document, and contains the weights of the different 	topics on different columns.  The last column of each row contains a unique document ID number (essentially reflecting the document order in the input matrix).\
These files are used by SOM_PAK's 'vsom' to train the SOM.  The different versions of the matrix in each file produced differ only by the order in which the documents appear.  The construction of these multiple training files permits running 'vsom' multiple times with different document orders.  The number of training files produced and other parameters are specified in tvtParameters.m.  More details about the parameters can be discovered by typing 'createSOM help' at the matlab prompt.\
\

\b 05createSOM+....txt
\b0 \
This tab-separated text file contains the data matrix required by SOM_PAK to create a Self Organizing Map (SOM).\
	1) The first row contains the number of columns (i.e., number of topics)\
	2) Each subsequent row corresponds to one document, and contains the weights of the different 	topics on different columns.  The last column of each row contains a unique document IDnumber 	(essentially reflecting the document order in the input matrix).\
This file is used by SOM_PAK's 'visual' to calibrate the SOM.\
\

\b 05creatSOM+...+rowsmetadata.tab
\b0 \
The tab-separated file contains a table with document names and metadata for each document:\
	1st row - field names (header)\
	1st column - document ID number (field: rowID)\
	2nd column - document name (field: rowName)\
	3rd-5th columns - document year, month, day\
	6th ... columns - document metadata fields\
This file is readable by ArcGIS as a table and the first field can be used to join this table with the objects (representing the documents) in a shape file.\
\

\b 05creatSOM+...+colsnames.tab
\b0 \
This tab-separated file contains a table with the top words for each topic:\
	1st row - field names (header)\
	1st column - topic ID number (field: columnID)\
	2nd column - topics top words (field: columnName)\
This file is readable by ArcGIS as a table and can be used to join topic words with the objects (representing the documents) in a shape file.\
\

\b 06sompak+...+docs.bmu
\b0 \
This file specifies the BMU (Best Matching Unit -- in this case, the SOM neuron) for each of the documents in the corpus and provides document coordinates to be used for visualization in ArcGIS.  The SOMAnalyst toolbox uses this .bmu file to create the .dbf, .shp, and .shx files for ArcGIS.\
\

\b 06sompak+...+docs.dbf
\b0 \
The database file that corresponds with the .shp file for document visualization in ArcGIS. \
\

\b 06sompak+...+docs.shp
\b0 \
The shapefile that describes the geometry of the document objects for visualization in ArcGIS.\
\

\b 06sompak+...+docs.shx
\b0 \
The index file that corresponds with the .shp file for document visualization in ArcGIS.\
\

\b 06sompak+...+neurons.cod
\b0 \
This file specifies the trained reference vectors with labels and provides neuron coordinates to be used for visualization in ArcGIS.\
\

\b 06sompak+...+neurons.dbf
\b0 \
The database file that corresponds with the .shp file for neuron visualization in ArcGIS.\
\

\b 06sompak+...+neurons.shp
\b0 \
The shapefile that describes the geometry of the neuron objects for visualization in ArcGIS.\
\

\b 06sompak+...+neurons.shx
\b0 \
The index file that corresponds with the .shp file for neuron visualization in ArcGIS.\
\

\b 07clusterDocs+....mat
\b0 \
This matlab file is created by cluster4SOM.m and contains the inputs to and outputs from the clustering algorithm.  The clustering method to be used and the file containing training data for supervised clustering (if desired) are specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'cluster4SOM help' at the matlab prompt.\
\

\b 07clusterDocs+....tab
\b0 \
This tab-separated text file is created by cluster4SOM.m and contains a table that specifies cluster membership for each document.  This table can be used in ArcGIS to label documents and/or choose document object symbology based on cluster membership.  The clustering method to be used and the file containing training data for supervised clustering (if desired) are specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'cluster4SOM help' at the matlab prompt.\
\

\b 07clusterDocs+...+log.txt
\b0 \
This text file is created by cluster4SOM.m and contains log outputs from the classifiers, including warnings, number of clusters identified, and confidence levels for supervised classifiers.\
\

\b 08clusterTopics+...+clusters_hierarchy.tab
\b0 \
This tab-separated text file is created by clusterTopics.m and contains a table that specifies cluster membership for each LDACOL topic.  The clustering method to be used is specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'clusterTopics help' at the matlab prompt.\
\

\b 08clusterTopics+...+clusters_manyeyes.tab
\b0 \
This tab-separated text file is created by clusterTopics.m and contains a table that can be used to visualize topic evolution over time with the manyeyes website (http://manyeyes.alphaworks.ibm.com/manyeyes/).\
\

\b 08clusterTopics+...+clusters_words.tab
\b0 \
This tab-separated text file is created by clusterTopics.m and contains a table that shows the top words of all of the metatopics (topic clusters) corresponding to each class. For hierarchical methods, this file includes the metatopics for all levels of the hierarchy.  The clustering method to be used is specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'clusterTopics help' at the matlab prompt.\
\

\b 09metaClasses+...+train.tab
\b0 \
This tab-separated text file is created by createClassesFromMetadata.m and contains a training set for the supervised classification of documents.  The metadata field to be used as the basis for class extraction is specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'createClassesFromMetadata help' at the matlab prompt.\
\

\b 09metaClasses+...+test.tab
\b0 \
This tab-separated text file is created by createClassesFromMetadata.m and contains a testing set for evaluating the accuracy of the supervised classification of documents.  The metadata field to be used as the basis for class extraction is specified in the tvtParameters.m file.  More details about the parameters can be discovered by typing 'createClassesFromMetadata help' at the matlab prompt.\
\

\b 10arcgisScripts+...+database.py
\b0 \
This python script is created by arcgisScripts.m and can be used to import a SOM map into ArcGIS.  This script will create an ArcGIS geodatabase with the following elements:\
	1) shape files corresponding to the SOM (including the documents' an neurons' positions)\
	2) a table with the documents' metadata\
	3) a table with the top words for each LDACOL topic\
	4) tables with the clustering results obtained from different runs of cluster4SOM.m.\
NOTE:  This script should be executed from outside ArcGIS and should be run before running 10arcgisScripts+...+layers.py.\
\

\b 10arcgisScripts+...+layers.py
\b0 \
This python script is created by arcgisScripts.m and can be used to import a SOM map into ArcGIS.  This script will create a set of ArcMap layers to visualize the corpus.  These layers include:\
	1) a layer containing the documents' shapefile joined with the metadata table (05creatSOM+...+rowsmetadata.tab) to visualize metadata properties (e.g., document year)\
	2) layers containing the documents' shapefile joined with the columns of the clustering table to visualize the document clustering results.  One layer is created for each column of the clustering table (07clusterDocs+....tab) that contains useful data.\
The layers created are simply the shapefiles joined with the appropriate tables; they DO NOT include symbology information, which will need to be set manually in ArcMap.  This means that, aside from the name, all layers that join the documents' shapefile with the same clustering table will actually have the same content.\
NOTE:  This script should be executed from outside ArcGIS and should be run after running 10arcgisScripts+...+database.py.\
\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\
Copyright (C) 2010  Stacy & Joao Hespanha\
\
This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\
\
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\
\
You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}